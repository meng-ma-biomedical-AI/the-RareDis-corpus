{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "brat_statistics.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "clk8Dp2h-0QX"
      },
      "source": [
        "# Statistics\n",
        "This notebooks allows you to obtain some statistics of the dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MDZlCuyS2jxg",
        "outputId": "bdc8f468-b439-4a0c-e67f-55a4213e0674"
      },
      "source": [
        "!pip install pyforest==0.1.1\n",
        "#https://pypi.org/project/pyforest/0.1.1/\n",
        "import pyforest  #With this library, you won't need to import more packages!!!\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting pyforest==0.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/ca/7a/2280448ba4202604eb3f9e23d9a4fd0ca1473d31aca0a90fdb5f31dd902c/pyforest-0.1.1.tar.gz (3.4MB)\n",
            "\u001b[K     |████████████████████████████████| 3.4MB 6.5MB/s \n",
            "\u001b[?25hBuilding wheels for collected packages: pyforest\n",
            "  Building wheel for pyforest (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pyforest: filename=pyforest-0.1.1-py2.py3-none-any.whl size=9213 sha256=e93e1b16570bceb4ee5e560d59e406fd016e03cef9e32bcd258082541327cd2d\n",
            "  Stored in directory: /root/.cache/pip/wheels/77/f9/78/51500678d6ce472b574216a40cba6c81d1766ee7cc838cce3c\n",
            "Successfully built pyforest\n",
            "Installing collected packages: pyforest\n",
            "Successfully installed pyforest-0.1.1\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uLgJ6C4H2C_J",
        "outputId": "86cbdc95-f19a-4200-f7b2-db07bd517ed6"
      },
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/drive/\")\n",
        "\n",
        "root='drive/My Drive/Colab Notebooks/nlp4rareNER/'\n",
        "path=root+'data/goldstandard/'\n",
        "\n",
        "\n",
        "train_path=path+'train/'\n",
        "dev_path=path+'dev/'\n",
        "test_path=path+'test/'\n",
        "\n",
        "print(\"training:\", train_path)\n",
        "print(\"dev:\", dev_path)\n",
        "print(\"test:\", test_path)"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n",
            "training: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/train/\n",
            "dev: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/dev/\n",
            "test: drive/My Drive/Colab Notebooks/nlp4rareNER/data/goldstandard/test/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DtMcXTN02VEQ"
      },
      "source": [
        "## Number of documents, sentences and tokens"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "he3lPxAE19gg",
        "outputId": "0d57d31e-75e2-4135-8522-fc9a1e4219fc"
      },
      "source": [
        "import glob\n",
        "size_train=len(glob.glob(train_path+'*.txt'))\n",
        "print('number of documents in training:',size_train)\n",
        "size_dev=len(glob.glob(dev_path+'*.txt'))\n",
        "print('number of documents in development:',size_dev)\n",
        "size_test=len(glob.glob(test_path+'*.txt'))\n",
        "print('number of documents in test:',size_test)\n",
        "total=size_train+size_dev+size_test\n",
        "print('total of documents:',total)\n"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "number of documents in training: 729\n",
            "number of documents in development: 104\n",
            "number of documents in test: 209\n",
            "total of documents: 1042\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "n68iJmww3aG-"
      },
      "source": [
        "We will install Spacy, a library NLP, that helps us to tokenize the texts."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "PYnegGK92aSw",
        "outputId": "feb78a92-b1ce-46a0-8c2c-9cb7cd92973e"
      },
      "source": [
        "!python -m spacy download en_core_web_sm #install spacy and download the model en_core_web_sm\n",
        "import spacy #NLP library for sentence segmentation and tokenization\n",
        "nlp = spacy.load(\"en_core_web_sm\")"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: en_core_web_sm==2.2.5 from https://github.com/explosion/spacy-models/releases/download/en_core_web_sm-2.2.5/en_core_web_sm-2.2.5.tar.gz#egg=en_core_web_sm==2.2.5 in /usr/local/lib/python3.7/dist-packages (2.2.5)\n",
            "Requirement already satisfied: spacy>=2.2.2 in /usr/local/lib/python3.7/dist-packages (from en_core_web_sm==2.2.5) (2.2.4)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.38.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (4.41.1)\n",
            "Requirement already satisfied: murmurhash<1.1.0,>=0.28.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: preshed<3.1.0,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.5)\n",
            "Requirement already satisfied: catalogue<1.1.0,>=0.0.7 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.0)\n",
            "Requirement already satisfied: numpy>=1.15.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.19.5)\n",
            "Requirement already satisfied: blis<0.5.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.4.1)\n",
            "Requirement already satisfied: thinc==7.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (7.4.0)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.13.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.23.0)\n",
            "Requirement already satisfied: srsly<1.1.0,>=1.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.0.5)\n",
            "Requirement already satisfied: cymem<2.1.0,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (2.0.5)\n",
            "Requirement already satisfied: setuptools in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (57.0.0)\n",
            "Requirement already satisfied: wasabi<1.1.0,>=0.4.0 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (0.8.2)\n",
            "Requirement already satisfied: plac<1.2.0,>=0.9.6 in /usr/local/lib/python3.7/dist-packages (from spacy>=2.2.2->en_core_web_sm==2.2.5) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata>=0.20; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (4.0.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2020.12.5)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.0.4)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (1.24.3)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests<3.0.0,>=2.13.0->spacy>=2.2.2->en_core_web_sm==2.2.5) (2.10)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.4.1)\n",
            "Requirement already satisfied: typing-extensions>=3.6.4; python_version < \"3.8\" in /usr/local/lib/python3.7/dist-packages (from importlib-metadata>=0.20; python_version < \"3.8\"->catalogue<1.1.0,>=0.0.7->spacy>=2.2.2->en_core_web_sm==2.2.5) (3.7.4.3)\n",
            "\u001b[38;5;2m✔ Download and installation successful\u001b[0m\n",
            "You can now load the model via spacy.load('en_core_web_sm')\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "h2MsOgZcGsak"
      },
      "source": [
        "def getText(path_file):\n",
        "    \"\"\"returns the text of the file path_file\"\"\"\n",
        "    f=open(path_file,'r')\n",
        "    text=f.read()\n",
        "    f.close()\n",
        "    return text\n",
        "\n",
        "def countTokens(path):\n",
        "    \"\"\"This functions takes a directory path where a dataset is stored, g\n",
        "    gets its txt files to parse them  by using Spacy \n",
        "    for obtaining the number of tokens and sentences in\n",
        "    the dataset\"\"\"\n",
        "    \n",
        "    total_sentences=0\n",
        "    total_tokens=0\n",
        "    files=glob.glob(path+'*.txt')\n",
        "    #print(path,len(files))\n",
        "    \n",
        "    for i,file_text in enumerate(files):\n",
        "        text=getText(file_text)\n",
        "        #print(file_text,text)\n",
        "\n",
        "        doc = nlp(text)\n",
        "        total_sentences+=len(list(doc.sents))\n",
        "        total_tokens+=len(doc)\n",
        "\n",
        "    return total_sentences, total_tokens\n",
        "\n",
        "sentences_train, tokens_train = countTokens(train_path)\n",
        "sentences_dev, tokens_dev = countTokens(dev_path)\n",
        "sentences_test, tokens_test = countTokens(test_path)\n",
        "\n",
        "print('Sentences and tokens in training: ', sentences_train, tokens_train)\n",
        "print('Sentences and tokens in dev: ', sentences_dev, tokens_dev)\n",
        "print('Sentences and tokens in text: ', sentences_test, tokens_test)\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nVhCbnN8DQEX"
      },
      "source": [
        "## Number of entities\n",
        "\n",
        "We now obtain the number of entities:"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "G3Tp5U7yALUp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "6b2bc29d-a16b-47db-bd08-f2372ac83389"
      },
      "source": [
        "\n",
        "\n",
        "def getAnnotations(path_file,allTypes=False):\n",
        "    \"\"\"returns a dataframe with its annotations\"\"\"\n",
        "    annotations=[]\n",
        "    try:  \n",
        "        f=open(path_file,'r')\n",
        "        annotations = f.readlines()       \n",
        "        f.close()\n",
        "    except:\n",
        "        pass\n",
        "\n",
        "    num_entities=0\n",
        "    total_dis=0\n",
        "    total_raredis=0\n",
        "    total_skin=0\n",
        "    total_sym=0\n",
        "    total_sign=0        \n",
        "\n",
        "\n",
        "    for ann in annotations:\n",
        "        if ann.startswith('T'):\n",
        "            idEnt=ann[:ann.index('\\t')]\n",
        "            typeEnt=ann[ann.index('\\t')+1:ann.index(' ')]\n",
        "            num_entities+=1\n",
        "\n",
        "            if typeEnt=='DISEASE':\n",
        "                if allTypes:\n",
        "                    total_dis+=1\n",
        "                else:\n",
        "                    num_entities-=1\n",
        "            elif typeEnt=='RAREDISEASE':\n",
        "                total_raredis+=1\n",
        "            elif typeEnt=='SKINRAREDISEASE':\n",
        "                total_skin+=1\n",
        "            elif typeEnt=='SYMPTOM':\n",
        "                total_sym+=1\n",
        "            elif typeEnt=='SIGN':\n",
        "                total_sign+=1\n",
        "            else:\n",
        "                #if allTypes==False\n",
        "                pass\n",
        "\n",
        "    return num_entities, total_dis, total_raredis, total_skin, total_sym, total_sign\n",
        "\n",
        "\n",
        "def countEntities(path):\n",
        "    total_entities=0\n",
        "    total_dis=0\n",
        "    total_rare=0\n",
        "    total_skin=0\n",
        "    total_sym=0\n",
        "    total_sign=0\n",
        "\n",
        "    files=glob.glob(path+'*.ann')\n",
        "    #print(path,len(files))\n",
        "    \n",
        "    for i,file_path in enumerate(files):\n",
        "        ne, td, tr, tsk, tsy, tsi=getAnnotations(file_path)\n",
        "        total_entities+=ne\n",
        "        total_dis+=td\n",
        "        total_rare+=tr\n",
        "        total_skin+=tsk\n",
        "        total_sym+=tsy\n",
        "        total_sign+=tsi\n",
        "\n",
        "    return total_entities, total_dis, total_rare, total_skin, total_sym, total_sign\n",
        "\n",
        "\n",
        "\n",
        "print('Entities,Diseases,Rare Diseases, Skin Rare Diseases, Symptoms, Signs')\n",
        "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign= countEntities(train_path)\n",
        "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n",
        "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign = countEntities(dev_path)\n",
        "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n",
        "\n",
        "total_entities, total_dis, total_rare, total_skin, total_sym, total_sign = countEntities(test_path)\n",
        "print(total_entities, total_dis, total_rare, total_skin, total_sym, total_sign)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Entities,Diseases,Rare Diseases, Skin Rare Diseases, Symptoms, Signs\n",
            "8774 0 3157 451 318 3742\n",
            "1228 0 480 45 24 528\n",
            "2478 0 942 146 53 1061\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}
